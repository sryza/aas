{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea671b27",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark\n",
    "\n",
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b6518af",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder.config(\"spark.driver.memory\", \"6g\").appName('chapter_3').getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c933831",
   "metadata": {},
   "source": [
    "### Setting Up the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ebf2e8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_user_artist_path = \"data/audioscrobbler_data/user_artist_data.txt\"\n",
    "raw_user_artist_data = spark.read.text(raw_user_artist_path)\n",
    "\n",
    "raw_user_artist_data.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3967e6cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_artist_data = spark.read.text(\"data/audioscrobbler_data/artist_data.txt\")\n",
    "\n",
    "raw_artist_data.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71031398",
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_artist_alias = spark.read.text(\"data/audioscrobbler_data/artist_alias.txt\")\n",
    "\n",
    "raw_artist_alias.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7717802f",
   "metadata": {},
   "source": [
    "### Preparing the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08016f94",
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_user_artist_data.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75d04310",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import split, min, max\n",
    "from pyspark.sql.types import IntegerType, StringType\n",
    "\n",
    "user_artist_df = raw_user_artist_data.withColumn('user',\n",
    "                                    split(raw_user_artist_data['value'], ' ').\\\n",
    "                                    getItem(0).\\\n",
    "                                    cast(IntegerType()))\n",
    "user_artist_df = user_artist_df.withColumn('artist',\n",
    "                                    split(raw_user_artist_data['value'], ' ').\\\n",
    "                                    getItem(1).\\\n",
    "                                    cast(IntegerType()))\n",
    "user_artist_df = user_artist_df.withColumn('count',\n",
    "                                    split(raw_user_artist_data['value'], ' ').\\\n",
    "                                    getItem(2).\\\n",
    "                                    cast(IntegerType())).drop('value')\n",
    "\n",
    "user_artist_df.select([min(\"user\"), max(\"user\"), min(\"artist\"),\\\n",
    "                                    max(\"artist\")]).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dae328d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col\n",
    "\n",
    "artist_by_id = raw_artist_data.withColumn('id', split(col('value'), '\\s+', 2).\\\n",
    "                                                getItem(0).\\\n",
    "                                                cast(IntegerType()))\n",
    "artist_by_id = artist_by_id.withColumn('name', split(col('value'), '\\s+', 2).\\\n",
    "                                               getItem(1).\\\n",
    "                                               cast(StringType())).drop('value')\n",
    "\n",
    "artist_by_id.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8706350",
   "metadata": {},
   "outputs": [],
   "source": [
    "artist_alias = raw_artist_alias.withColumn('artist',\n",
    "                                          split(col('value'), '\\s+').\\\n",
    "                                                getItem(0).\\\n",
    "                                                cast(IntegerType())).\\\n",
    "                                withColumn('alias',\n",
    "                                            split(col('value'), '\\s+').\\\n",
    "                                            getItem(1).\\\n",
    "                                            cast(StringType())).\\\n",
    "                                drop('value')\n",
    "\n",
    "artist_alias.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54e75fa4",
   "metadata": {},
   "outputs": [],
   "source": [
    "artist_by_id.filter(artist_by_id.id.isin(1092764, 1000311)).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89707c76",
   "metadata": {},
   "source": [
    "### Building a First Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36c11890",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import broadcast, when\n",
    "\n",
    "train_data = user_artist_df.join(broadcast(artist_alias),\n",
    "                                              'artist', how='left')\n",
    "train_data = train_data.withColumn('artist',\n",
    "                                    when(col('alias').isNull(), col('artist')).\\\n",
    "                                    otherwise(col('alias')))\n",
    "\n",
    "train_data = train_data.withColumn('artist', col('artist').\\\n",
    "                                             cast(IntegerType())).\\\n",
    "                                             drop('alias')\n",
    "\n",
    "train_data.cache()\n",
    "\n",
    "train_data.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0169aa94",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.recommendation import ALS\n",
    "\n",
    "model = ALS(rank=10, seed=0, maxIter=5, regParam=0.1,\n",
    "            implicitPrefs=True, alpha=1.0, userCol='user',\n",
    "            itemCol='artist', ratingCol='count'). \\\n",
    "        fit(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf83dc48",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.userFactors.show(1, truncate = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17b0acc2",
   "metadata": {},
   "source": [
    "### Spot Checking Recommendations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4322374a",
   "metadata": {},
   "outputs": [],
   "source": [
    "user_id = 2093760\n",
    "\n",
    "existing_artist_ids = train_data.filter(train_data.user == user_id).select(\"artist\").collect()\n",
    "\n",
    "existing_artist_ids = [i[0] for i in existing_artist_ids]\n",
    "\n",
    "artist_by_id.filter(col('id').isin(existing_artist_ids)).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1feacaef",
   "metadata": {},
   "outputs": [],
   "source": [
    "user_subset = train_data.select('user').where(col('user') == user_id).distinct()\n",
    "top_predictions = model.recommendForUserSubset(user_subset, 5)\n",
    "\n",
    "top_predictions.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98f6bed1",
   "metadata": {},
   "outputs": [],
   "source": [
    "top_predictions_pandas = top_predictions.toPandas()\n",
    "print(top_predictions_pandas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0baa4ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "recommended_artist_ids = [i[0] for i in top_predictions_pandas.\\\n",
    "                                        recommendations[0]]\n",
    "\n",
    "artist_by_id.filter(col('id').isin(recommended_artist_ids)).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4c6168e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, lit, count, mean, coalesce\n",
    "from pyspark.sql import DataFrame\n",
    "from typing import List\n",
    "import random\n",
    "\n",
    "\n",
    "def area_under_curve(positive_data: DataFrame, b_all_artist_ids: List[int], predict_function) -> float:\n",
    "    positive_predictions = predict_function(positive_data.select(\"user\", \"artist\")).withColumnRenamed(\"prediction\", \"positivePrediction\")\n",
    "    \n",
    "    def negative_data_generation(user_artist_tuples):\n",
    "        user_negative_artists = []\n",
    "        for user, pos_artist_ids in user_artist_tuples:\n",
    "            pos_artist_id_set = set(pos_artist_ids)\n",
    "            negative_artists = set()\n",
    "            while len(negative_artists) < len(pos_artist_id_set):\n",
    "                artist_id = b_all_artist_ids[random.randint(0, len(b_all_artist_ids) - 1)]\n",
    "                if artist_id not in pos_artist_id_set:\n",
    "                    negative_artists.add(artist_id)\n",
    "            user_negative_artists.extend([(user, artist_id) for artist_id in negative_artists])\n",
    "        return user_negative_artists\n",
    "    \n",
    "    user_artist_rdd = positive_data.select(\"user\", \"artist\").rdd.groupByKey().mapValues(list).collect()\n",
    "    negative_data = spark.createDataFrame(negative_data_generation(user_artist_rdd), schema=[\"user\", \"artist\"])\n",
    "    \n",
    "    negative_predictions = predict_function(negative_data).withColumnRenamed(\"prediction\", \"negativePrediction\")\n",
    "    \n",
    "    joined_predictions = positive_predictions.join(negative_predictions, \"user\").select(\"user\", \"positivePrediction\", \"negativePrediction\").cache()\n",
    "    \n",
    "    all_counts = joined_predictions.groupBy(\"user\").agg(count(lit(1)).alias(\"total\")).select(\"user\", \"total\")\n",
    "    correct_counts = joined_predictions.filter(col(\"positivePrediction\") > col(\"negativePrediction\")).groupBy(\"user\").agg(count(\"user\").alias(\"correct\")).select(\"user\", \"correct\")\n",
    "    \n",
    "    mean_auc = all_counts.join(correct_counts, [\"user\"], \"left_outer\").select(col(\"user\"), (coalesce(col(\"correct\"), lit(0)) / col(\"total\")).alias(\"auc\")).agg(mean(\"auc\")).collect()[0][0]\n",
    "    \n",
    "    joined_predictions.unpersist()\n",
    "    \n",
    "    return mean_auc\n",
    "\n",
    "\n",
    "\n",
    "all_data = user_artist_df.join(broadcast(artist_alias), 'artist', how='left') \\\n",
    "    .withColumn('artist', when(col('alias').isNull(), col('artist'))\\\n",
    "    .otherwise(col('alias'))) \\\n",
    "    .withColumn('artist', col('artist').cast(IntegerType())).drop('alias')\n",
    "\n",
    "train_data, cv_data = all_data.randomSplit([0.9, 0.1], seed=54321)\n",
    "train_data.cache()\n",
    "cv_data.cache()\n",
    "\n",
    "all_artist_ids = all_data.select(\"artist\").distinct()\n",
    "all_artist_ids = [i[0] for i in all_artist_ids.collect()]\n",
    "# b_all_artist_ids = broadcast(all_artist_ids)\n",
    "\n",
    "model = ALS(rank=10, seed=0, maxIter=5, regParam=0.1,\n",
    "            implicitPrefs=True, alpha=1.0, userCol='user',\n",
    "            itemCol='artist', ratingCol='count') \\\n",
    "        .fit(train_data)\n",
    "\n",
    "area_under_curve(cv_data, all_artist_ids, model.transform)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce2916a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import sum as _sum\n",
    "\n",
    "def predict_most_listened(train):\n",
    "    listen_counts = train.groupBy(\"artist\").agg(_sum(\"count\").alias(\"prediction\")).select(\"artist\", \"prediction\")\n",
    "    return train.join(listen_counts, \"artist\", \"left_outer\").select(\"user\", \"artist\", \"prediction\")\n",
    "\n",
    "\n",
    "area_under_curve(cv_data, all_artist_ids, predict_most_listened)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "486f3b4f",
   "metadata": {},
   "source": [
    "### Hyperparameter selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4f1447a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pprint import pprint\n",
    "from itertools import product\n",
    "\n",
    "ranks = [5, 30]\n",
    "reg_params = [4.0, 0.0001]\n",
    "alphas = [1.0, 40.0]\n",
    "hyperparam_combinations = list(product(*[ranks, reg_params, alphas]))\n",
    "\n",
    "evaluations = []\n",
    "\n",
    "for c in hyperparam_combinations:\n",
    "    rank = c[0]\n",
    "    reg_param = c[1]\n",
    "    alpha = c[2]\n",
    "    model = ALS().setSeed(0).setImplicitPrefs(True).setRank(rank).setRegParam(reg_param).setAlpha(alpha).setMaxIter(20).setUserCol(\"user\").setItemCol(\"artist\").setRatingCol(\"count\").setPredictionCol(\"prediction\").fit(train_data)\n",
    "\n",
    "    auc = area_under_curve(cv_data, all_artist_ids, model.transform)\n",
    "\n",
    "    model.userFactors.unpersist()\n",
    "    model.itemFactors.unpersist()\n",
    "\n",
    "    evaluations.append((auc, (rank, reg_param, alpha)))\n",
    "\n",
    "evaluations.sort(key=lambda x: x[0], reverse=True)\n",
    "pprint(evaluations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04f1325f",
   "metadata": {},
   "outputs": [],
   "source": [
    "some_users = all_data.select(\"user\").distinct().limit(100)\n",
    "\n",
    "def make_recommendations(model, user_id, num_recs):\n",
    "    user_subset = train_data.select('user').where(col('user') == user_id).distinct()\n",
    "    recommendations = model.recommendForUserSubset(user_subset, num_recs)\n",
    "    return recommendations\n",
    "\n",
    "some_recommendations = [(user_id[0], make_recommendations(model, user_id[0], 5)) for user_id in some_users.collect()]\n",
    "\n",
    "for user_id, recs_df in some_recommendations:\n",
    "    recs_df = recs_df.select(\"recommendations\")\n",
    "    recommended_artists = [row.asDict()[\"artist\"] for row in recs_df.collect()[0][0]]\n",
    "    print(f\"{user_id} -> {', '.join(map(str, recommended_artists))}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
