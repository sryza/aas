{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark\n",
    "\n",
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder.config(\"spark.driver.memory\", \"6g\").appName('chapter_5').getOrCreate()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Identifying Anomalous Network Traffic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!head -n 1 data/kddcup.data"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A First Take on Clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_without_header = spark.read.option(\"inferSchema\", True).\\\n",
    "                                  option(\"header\", False).\\\n",
    "                                  csv(\"data/kddcup.data\")\n",
    "\n",
    "column_names = [  \"duration\", \"protocol_type\", \"service\", \"flag\",\n",
    "  \"src_bytes\", \"dst_bytes\", \"land\", \"wrong_fragment\", \"urgent\",\n",
    "  \"hot\", \"num_failed_logins\", \"logged_in\", \"num_compromised\",\n",
    "  \"root_shell\", \"su_attempted\", \"num_root\", \"num_file_creations\",\n",
    "  \"num_shells\", \"num_access_files\", \"num_outbound_cmds\",\n",
    "  \"is_host_login\", \"is_guest_login\", \"count\", \"srv_count\",\n",
    "  \"serror_rate\", \"srv_serror_rate\", \"rerror_rate\", \"srv_rerror_rate\",\n",
    "  \"same_srv_rate\", \"diff_srv_rate\", \"srv_diff_host_rate\",\n",
    "  \"dst_host_count\", \"dst_host_srv_count\",\n",
    "  \"dst_host_same_srv_rate\", \"dst_host_diff_srv_rate\",\n",
    "  \"dst_host_same_src_port_rate\", \"dst_host_srv_diff_host_rate\",\n",
    "  \"dst_host_serror_rate\", \"dst_host_srv_serror_rate\",\n",
    "  \"dst_host_rerror_rate\", \"dst_host_srv_rerror_rate\",\n",
    "  \"label\"]\n",
    "\n",
    "data = data_without_header.toDF(*column_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col\n",
    "data.select(\"label\").groupBy(\"label\").count().\\\n",
    "      orderBy(col(\"count\").desc()).show(25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.ml.clustering import KMeans, KMeansModel\n",
    "from pyspark.ml import Pipeline\n",
    "\n",
    "numeric_only = data.drop(\"protocol_type\", \"service\", \"flag\").cache()\n",
    "\n",
    "assembler = VectorAssembler().setInputCols(numeric_only.columns[:-1]).\\\n",
    "                              setOutputCol(\"featureVector\")\n",
    "\n",
    "kmeans = KMeans().setPredictionCol(\"cluster\").setFeaturesCol(\"featureVector\")\n",
    "\n",
    "pipeline = Pipeline().setStages([assembler, kmeans])\n",
    "pipeline_model = pipeline.fit(numeric_only)\n",
    "kmeans_model = pipeline_model.stages[1]\n",
    "\n",
    "from pprint import pprint\n",
    "pprint(kmeans_model.clusterCenters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with_cluster = pipeline_model.transform(numeric_only)\n",
    "\n",
    "with_cluster.select(\"cluster\", \"label\").groupBy(\"cluster\", \"label\").count().\\\n",
    "              orderBy(col(\"cluster\"), col(\"count\").desc()).show(25)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Choosing k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import DataFrame\n",
    "from random import randint\n",
    "\n",
    "def clustering_score(input_data, k):\n",
    "    input_numeric_only = input_data.drop(\"protocol_type\", \"service\", \"flag\")\n",
    "    assembler = VectorAssembler().setInputCols(input_numeric_only.columns[:-1]).setOutputCol(\"featureVector\")\n",
    "    kmeans = KMeans().setSeed(randint(100,100000)).setK(k).setPredictionCol(\"cluster\").setFeaturesCol(\"featureVector\")\n",
    "    pipeline = Pipeline().setStages([assembler, kmeans])\n",
    "    pipeline_model = pipeline.fit(input_numeric_only)\n",
    "    kmeans_model = pipeline_model.stages[-1]\n",
    "    training_cost = kmeans_model.summary.trainingCost\n",
    "    return training_cost\n",
    "\n",
    "for k in list(range(20,100, 20)):\n",
    "    print(clustering_score(numeric_only, k))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clustering_score_1(input_data, k):\n",
    "    input_numeric_only = input_data.drop(\"protocol_type\", \"service\", \"flag\")\n",
    "    assembler = VectorAssembler().\\\n",
    "                  setInputCols(input_numeric_only.columns[:-1]).\\\n",
    "                  setOutputCol(\"featureVector\")\n",
    "    kmeans = KMeans().setSeed(randint(100,100000)).setK(k).setMaxIter(40).\\\n",
    "      setTol(1.0e-5).\\\n",
    "      setPredictionCol(\"cluster\").setFeaturesCol(\"featureVector\")\n",
    "    pipeline = Pipeline().setStages([assembler, kmeans])\n",
    "    pipeline_model = pipeline.fit(input_numeric_only)\n",
    "    kmeans_model = pipeline_model.stages[-1]\n",
    "    training_cost = kmeans_model.summary.trainingCost\n",
    "    return training_cost\n",
    "\n",
    "\n",
    "for k in list(range(20,101, 20)):\n",
    "    print(k, clustering_score_1(numeric_only, k))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import StandardScaler\n",
    "\n",
    "def clustering_score_2(input_data, k):\n",
    "    input_numeric_only = input_data.drop(\"protocol_type\", \"service\", \"flag\")\n",
    "    assembler = VectorAssembler().\\\n",
    "                setInputCols(input_numeric_only.columns[:-1]).\\\n",
    "                setOutputCol(\"featureVector\")\n",
    "    scaler = StandardScaler().setInputCol(\"featureVector\").\\\n",
    "                              setOutputCol(\"scaledFeatureVector\").\\\n",
    "                              setWithStd(True).setWithMean(False)\n",
    "    kmeans = KMeans().setSeed(randint(100,100000)).\\\n",
    "                      setK(k).setMaxIter(40).\\\n",
    "                      setTol(1.0e-5).setPredictionCol(\"cluster\").\\\n",
    "                      setFeaturesCol(\"scaledFeatureVector\")\n",
    "    pipeline = Pipeline().setStages([assembler, scaler, kmeans])\n",
    "    pipeline_model = pipeline.fit(input_numeric_only)\n",
    "    kmeans_model = pipeline_model.stages[-1]\n",
    "    training_cost = kmeans_model.summary.trainingCost\n",
    "    return training_cost\n",
    "\n",
    "for k in list(range(60, 271, 30)):\n",
    "    print(k, clustering_score_2(numeric_only, k))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Categorical Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import OneHotEncoder, StringIndexer\n",
    "\n",
    "def one_hot_pipeline(input_col):\n",
    "    indexer = StringIndexer().setInputCol(input_col).setOutputCol(input_col + \"_indexed\")\n",
    "    encoder = OneHotEncoder().setInputCol(input_col + \"_indexed\").setOutputCol(input_col + \"_vec\")\n",
    "    pipeline = Pipeline().setStages([indexer, encoder])\n",
    "    return pipeline, input_col + \"_vec\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clustering_score_3(input_data, k):\n",
    "    proto_type_pipeline, proto_type_vec_col = one_hot_pipeline(\"protocol_type\")\n",
    "    service_pipeline, service_vec_col = one_hot_pipeline(\"service\")\n",
    "    flag_pipeline, flag_vec_col = one_hot_pipeline(\"flag\")\n",
    "\n",
    "    assemble_cols = set(input_data.columns) - \\\n",
    "                    {\"label\", \"protocol_type\", \"service\", \"flag\"} | \\\n",
    "                    {proto_type_vec_col, service_vec_col, flag_vec_col}\n",
    "\n",
    "    assembler = VectorAssembler().setInputCols(list(assemble_cols)).setOutputCol(\"featureVector\")\n",
    "    scaler = StandardScaler().setInputCol(\"featureVector\").setOutputCol(\"scaledFeatureVector\").setWithStd(True).setWithMean(False)\n",
    "    kmeans = KMeans().setSeed(randint(100,100000)).setK(k).setMaxIter(40).setTol(1.0e-5).setPredictionCol(\"cluster\").setFeaturesCol(\"scaledFeatureVector\")\n",
    "    pipeline = Pipeline().setStages([proto_type_pipeline, service_pipeline, flag_pipeline, assembler, scaler, kmeans])\n",
    "    pipeline_model = pipeline.fit(input_data)\n",
    "\n",
    "    kmeans_model = pipeline_model.stages[-1]\n",
    "    training_cost = kmeans_model.summary.trainingCost\n",
    "    return training_cost\n",
    "\n",
    "for k in list(range(60, 271, 30)):\n",
    "    print(k, clustering_score_3(data, k))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using Labels with Entropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from math import log\n",
    "\n",
    "def entropy(counts):\n",
    "    values = [c for c in counts if (c > 0)]\n",
    "    n = sum(values)\n",
    "    p = [v/n for v in values]\n",
    "    return sum([-1*(p_v) * log(p_v) for p_v in p])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as fun\n",
    "from pyspark.sql import Window\n",
    "\n",
    "cluster_label = pipeline_model.\\\n",
    "                    transform(data).\\\n",
    "                    select(\"cluster\", \"label\")\n",
    "\n",
    "df = cluster_label.\\\n",
    "        groupBy(\"cluster\", \"label\").\\\n",
    "        count().orderBy(\"cluster\")\n",
    "\n",
    "w = Window.partitionBy(\"cluster\")\n",
    "\n",
    "p_col = df['count'] / fun.sum(df['count']).over(w)\n",
    "with_p_col = df.withColumn(\"p_col\", p_col)\n",
    "\n",
    "result = with_p_col.groupBy(\"cluster\").\\\n",
    "              agg(-fun.sum(col(\"p_col\") * fun.log2(col(\"p_col\")))\\\n",
    "                        .alias(\"entropy\"),\n",
    "                    fun.sum(col(\"count\"))\\\n",
    "                        .alias(\"cluster_size\"))\n",
    "\n",
    "result = result.withColumn('weightedClusterEntropy',\n",
    "                          fun.col('entropy') * fun.col('cluster_size'))\n",
    "\n",
    "weighted_cluster_entropy_avg = result.\\\n",
    "                            agg(fun.sum(\n",
    "                              col('weightedClusterEntropy'))).\\\n",
    "                            collect()\n",
    "weighted_cluster_entropy_avg[0][0]/data.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_pipeline_4(data, k):\n",
    "    (proto_type_pipeline, proto_type_vec_col) = one_hot_pipeline(\"protocol_type\")\n",
    "    (service_pipeline, service_vec_col) = one_hot_pipeline(\"service\")\n",
    "    (flag_pipeline, flag_vec_col) = one_hot_pipeline(\"flag\")\n",
    "\n",
    "    assemble_cols = set(data.columns) - {\"label\", \"protocol_type\", \"service\", \"flag\"} | {proto_type_vec_col, service_vec_col, flag_vec_col}\n",
    "    assembler = VectorAssembler(inputCols=list(assemble_cols), outputCol=\"featureVector\")\n",
    "\n",
    "    scaler = StandardScaler(inputCol=\"featureVector\", outputCol=\"scaledFeatureVector\", withStd=True, withMean=False)\n",
    "\n",
    "    kmeans = KMeans(seed=randint(100, 100000), k=k, predictionCol=\"cluster\", featuresCol=\"scaledFeatureVector\", maxIter=40, tol=1.0e-5)\n",
    "\n",
    "    pipeline = Pipeline(stages=[proto_type_pipeline, service_pipeline, flag_pipeline, assembler, scaler, kmeans])\n",
    "    return pipeline.fit(data)\n",
    "\n",
    "\n",
    "def clustering_score_4(input_data, k):\n",
    "    pipeline_model = fit_pipeline_4(input_data, k)\n",
    "    cluster_label = pipeline_model.transform(input_data).select(\"cluster\", \"label\")\n",
    "\n",
    "    df = cluster_label.groupBy(\"cluster\", \"label\").count().orderBy(\"cluster\")\n",
    "\n",
    "    w = Window.partitionBy(\"cluster\")\n",
    "\n",
    "    p_col = df['count'] / fun.sum(df['count']).over(w)\n",
    "    with_p_col = df.withColumn(\"p_col\", p_col)\n",
    "\n",
    "    result = with_p_col.groupBy(\"cluster\").agg(-fun.sum(col(\"p_col\") * fun.log2(col(\"p_col\"))).alias(\"entropy\"),\n",
    "                                                fun.sum(col(\"count\")).alias(\"cluster_size\"))\n",
    "\n",
    "    result = result.withColumn('weightedClusterEntropy', col('entropy') * col('cluster_size'))\n",
    "\n",
    "    weighted_cluster_entropy_avg = result.agg(fun.sum(col('weightedClusterEntropy'))).collect()\n",
    "    return weighted_cluster_entropy_avg[0][0] / input_data.count()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clustering in Action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline_model = fit_pipeline_4(data, 180)\n",
    "count_by_cluster_label = pipeline_model.transform(data).\\\n",
    "                                        select(\"cluster\", \"label\").\\\n",
    "                                        groupBy(\"cluster\", \"label\").\\\n",
    "                                        count().orderBy(\"cluster\", \"label\")\n",
    "count_by_cluster_label.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "from pyspark.ml.linalg import Vector, Vectors\n",
    "from pyspark.sql.functions import udf\n",
    "\n",
    "k_means_model = pipeline_model.stages[-1]\n",
    "centroids = k_means_model.clusterCenters\n",
    "\n",
    "clustered = pipeline_model.transform(data)\n",
    "\n",
    "def dist_func(cluster, vec):\n",
    "    return float(np.linalg.norm(centroids[cluster] - vec))\n",
    "dist = udf(dist_func)\n",
    "\n",
    "threshold = clustered.select(\"cluster\", \"scaledFeatureVector\").\\\n",
    "    withColumn(\"dist_value\",\n",
    "        dist(col(\"cluster\"), col(\"scaledFeatureVector\"))).\\\n",
    "    orderBy(col(\"dist_value\").desc()).take(100)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
